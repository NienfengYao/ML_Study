{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic\n",
    "* Case1: First, I study from [VGG in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/) and implement it in [vgg16_pretrained_predict.ipynb](../vgg16_pretrained_predict.ipynb).\n",
    "  * Download the pretrained parameters and using the VGG16 model to predict, it works.\n",
    "* Case2: Then, I study from [Ashing00/Tensorflow_VGG](https://github.com/Ashing00/Tensorflow_VGG) and implement it in [vgg_cifar10_train.ipynb](../vgg16_cifar10_train/vgg_cifar10_train.ipynb)\n",
    "  * We using VGG16 modes (but the model has little different with Case1) to do training with CIFAR-10 dataset, it works.\n",
    "* There are some little different in the two VGG16 model \n",
    "  * Case 1 \n",
    "    * Do pre-process in data input, minus the mean value of RGB\n",
    "    * Pooling kernel size = 2x2, stride = 2\n",
    "  * Case 2\n",
    "    * Didn't the pre-process in data input. But it do batch normalization in each hiddle layer.\n",
    "    * Pooling kernel size = 2x2, stride = 2, except The Pool_1 (kernel size = 3x3, stride = 1)\n",
    "* Issue:\n",
    "  The training is failed, we can't get good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from vgg16_cifar10 import vgg16\n",
    "\n",
    "data_dir = \"../../../data/\"\n",
    "extract_folder = 'cifar-10-batches-bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(y, k):\n",
    "    \"\"\"\n",
    "    Encode labels into one-hot representation\n",
    "    y: the items.\n",
    "    k: the class number.\n",
    "    \"\"\"\n",
    "    onehot = np.zeros((y.shape[0], k))\n",
    "    for idx, val in enumerate(y):\n",
    "        onehot[idx,val] = 1.0  ##idx=0~xxxxx，if val =3 ,表示欄位3要設成1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image data from binary files. (train/test)\n",
    "\n",
    "def load_train_data(n):    # n=1,2..5, data_batch_1.bin ~ data_batch_5.bin\n",
    "    \"\"\"Load Cifar10 data from `path`\"\"\"\n",
    "    images_path = os.path.join(data_dir, extract_folder, 'data_batch_{}.bin'.format(n)) \n",
    "    return _load_binary_data(images_path)\n",
    "\n",
    "def load_test_data():      # test_batch\n",
    "    \"\"\"Load Cifar10 test data from `path`\"\"\"\n",
    "    test_path = os.path.join(data_dir, extract_folder, 'test_batch.bin') \n",
    "    return _load_binary_data(test_path)\n",
    "\n",
    "def _load_binary_data(path):\n",
    "    with open(path, 'rb') as img_file:\n",
    "        images = np.fromfile(img_file, dtype=np.uint8)\n",
    "    return images    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "MODEL_SAVE_PATH = \"./vgg/\"\n",
    "MODEL_NAME = \"vgg_cifar_model\"\n",
    "learning_rate = 0.001\n",
    "BATCH_SIZE = 120\n",
    "display_step = 100\n",
    "TRAINING_STEPS = 6000\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3072 # cifar data input (img shape: 32x32x3)\n",
    "n_classes = 10 # cifar10 total classes (0-9 )\n",
    "# Ryan mark\n",
    "# dropout = 0.60 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "def train(X_train, y_train_lable):\n",
    "    shuffle = True\n",
    "    batch_idx = 0\n",
    "    batch_len = int( X_train.shape[0] / BATCH_SIZE)\n",
    "    print(\"batch_len=\", batch_len) # 50000/120 = 416.6 => 416\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_idx = np.random.permutation(batch_len)    # 打散資料順序\n",
    "\n",
    "    # tf Graph input\n",
    "    x_ = tf.placeholder(tf.float32, [None, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    # Ryan mark\n",
    "    # keep_prob = tf.placeholder(tf.float32)    # dropout (keep probability)\n",
    "    x = tf.reshape(x_, shape=[-1, 32, 32, 3])\n",
    "\n",
    "    # Construct model\n",
    "    mean = list(np.mean(np.reshape(X_train, (-1, 32, 32, 3)) , axis=(0, 1, 2)))\n",
    "    print(\"mean:\", mean)\n",
    "    vgg = vgg16(x, mean=mean)\n",
    "    pred = vgg.fc3l\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    #GradientDescentOptimizer\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # 初始化TensorFlow持久化類。\n",
    "    saver = tf.train.Saver()\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        step = 1\n",
    "        print (\"Start  training!\")\n",
    "        # Keep training until reach max iterations:\n",
    "        while step < TRAINING_STEPS:\n",
    "            #batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            if shuffle==True:\n",
    "                batch_shuffle_idx=train_idx[batch_idx]\n",
    "                batch_xs=X_train[batch_shuffle_idx*BATCH_SIZE:batch_shuffle_idx*BATCH_SIZE+BATCH_SIZE]\n",
    "                batch_ys=y_train_lable[batch_shuffle_idx*BATCH_SIZE:batch_shuffle_idx*BATCH_SIZE+BATCH_SIZE]\n",
    "            else:\n",
    "                batch_xs=X_train[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE]\n",
    "                batch_ys=y_train_lable[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE]\n",
    "\n",
    "            if batch_idx<batch_len:\n",
    "                batch_idx+=1\n",
    "                if batch_idx==batch_len:\n",
    "                    batch_idx=0\n",
    "            else:\n",
    "                batch_idx=0\n",
    "            # Ryan: maybe we don't need the reshape? No, We need it, because of the input feature in conv1_1 is 3\n",
    "            reshaped_xs = np.reshape(batch_xs, (BATCH_SIZE, 32, 32, 3))\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(optimizer, feed_dict={x: reshaped_xs, y: batch_ys})\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: reshaped_xs, y: batch_ys})\n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(acc)\n",
    "            if step % display_step == 0:\n",
    "                print(\"Step: \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                    \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                    \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "        print(\"Optimization Finished!\")\n",
    "        print(\"Save model...\")\n",
    "        saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME))\n",
    "\n",
    "#         plt.subplot(1,2,1)\n",
    "#         plt.plot(train_loss)\n",
    "#         plt.xlabel('Iter')\n",
    "#         plt.ylabel('loss')\n",
    "#         plt.title('lr=%f, ti=%d, bs=%d' % (learning_rate, TRAINING_STEPS, BATCH_SIZE))\n",
    "#         #plt.tight_layout()\n",
    "\n",
    "#         plt.subplot(1,2,2)\n",
    "#         plt.plot(train_acc)\n",
    "#         plt.xlabel('Iter')\n",
    "#         plt.ylabel('accuracy')\n",
    "#         plt.title('lr=%f, ti=%d, bs=%d' % (learning_rate, TRAINING_STEPS, BATCH_SIZE))\n",
    "#         #plt.tight_layout()\n",
    "#         plt.savefig('vgg_cifar10_acc.jpg', dpi=200)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30730000,)\n",
      "(153650000,)\n",
      "X_train_image.shape = (50000, 3072)\n",
      "X_train_lable.shape = (50000, 10)\n",
      "batch_len= 416\n",
      "mean: [125.306918046875, 122.950394140625, 113.86538318359375]\n",
      "WARNING:tensorflow:From <ipython-input-5-9344fc4bae24>:26: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Start  training!\n",
      "Step: 100, Minibatch Loss= 4177.041016, Training Accuracy= 0.16667\n",
      "Step: 200, Minibatch Loss= 2121.913330, Training Accuracy= 0.20833\n",
      "Step: 300, Minibatch Loss= 1248.554688, Training Accuracy= 0.15000\n",
      "Step: 400, Minibatch Loss= 898.462891, Training Accuracy= 0.13333\n",
      "Step: 500, Minibatch Loss= 620.257080, Training Accuracy= 0.22500\n",
      "Step: 600, Minibatch Loss= 314.033478, Training Accuracy= 0.15833\n",
      "Step: 700, Minibatch Loss= 12.636030, Training Accuracy= 0.15000\n",
      "Step: 800, Minibatch Loss= 3.156986, Training Accuracy= 0.09167\n",
      "Step: 900, Minibatch Loss= 3.656419, Training Accuracy= 0.18333\n",
      "Step: 1000, Minibatch Loss= 2.333468, Training Accuracy= 0.13333\n",
      "Step: 1100, Minibatch Loss= 3.084056, Training Accuracy= 0.07500\n",
      "Step: 1200, Minibatch Loss= 2.352000, Training Accuracy= 0.10000\n",
      "Step: 1300, Minibatch Loss= 2.476279, Training Accuracy= 0.10833\n",
      "Step: 1400, Minibatch Loss= 2.638531, Training Accuracy= 0.10833\n",
      "Step: 1500, Minibatch Loss= 2.390403, Training Accuracy= 0.15000\n",
      "Step: 1600, Minibatch Loss= 2.863913, Training Accuracy= 0.09167\n",
      "Step: 1700, Minibatch Loss= 2.540984, Training Accuracy= 0.15833\n",
      "Step: 1800, Minibatch Loss= 2.349249, Training Accuracy= 0.15000\n",
      "Step: 1900, Minibatch Loss= 2.425092, Training Accuracy= 0.12500\n",
      "Step: 2000, Minibatch Loss= 2.359041, Training Accuracy= 0.08333\n",
      "Step: 2100, Minibatch Loss= 2.518902, Training Accuracy= 0.12500\n",
      "Step: 2200, Minibatch Loss= 2.507390, Training Accuracy= 0.17500\n",
      "Step: 2300, Minibatch Loss= 2.487643, Training Accuracy= 0.16667\n",
      "Step: 2400, Minibatch Loss= 2.421290, Training Accuracy= 0.12500\n",
      "Step: 2500, Minibatch Loss= 2.482033, Training Accuracy= 0.11667\n",
      "Step: 2600, Minibatch Loss= 2.392061, Training Accuracy= 0.12500\n",
      "Step: 2700, Minibatch Loss= 2.403667, Training Accuracy= 0.09167\n",
      "Step: 2800, Minibatch Loss= 2.401268, Training Accuracy= 0.10833\n",
      "Step: 2900, Minibatch Loss= 2.539561, Training Accuracy= 0.10833\n",
      "Step: 3000, Minibatch Loss= 2.394240, Training Accuracy= 0.07500\n",
      "Step: 3100, Minibatch Loss= 2.403811, Training Accuracy= 0.14167\n",
      "Step: 3200, Minibatch Loss= 2.457787, Training Accuracy= 0.12500\n",
      "Step: 3300, Minibatch Loss= 2.587118, Training Accuracy= 0.15000\n",
      "Step: 3400, Minibatch Loss= 3.007996, Training Accuracy= 0.10833\n",
      "Step: 3500, Minibatch Loss= 3.214185, Training Accuracy= 0.10833\n",
      "Step: 3600, Minibatch Loss= 3.090209, Training Accuracy= 0.07500\n",
      "Step: 3700, Minibatch Loss= 2.819278, Training Accuracy= 0.12500\n",
      "Step: 3800, Minibatch Loss= 2.490062, Training Accuracy= 0.15833\n",
      "Step: 3900, Minibatch Loss= 3.433897, Training Accuracy= 0.08333\n",
      "Step: 4000, Minibatch Loss= 3.730640, Training Accuracy= 0.04167\n",
      "Step: 4100, Minibatch Loss= 2.613277, Training Accuracy= 0.16667\n",
      "Step: 4200, Minibatch Loss= 7.944087, Training Accuracy= 0.06667\n",
      "Step: 4300, Minibatch Loss= 4.073916, Training Accuracy= 0.13333\n",
      "Step: 4400, Minibatch Loss= 3.006277, Training Accuracy= 0.11667\n",
      "Step: 4500, Minibatch Loss= 2.554006, Training Accuracy= 0.13333\n",
      "Step: 4600, Minibatch Loss= 2.472013, Training Accuracy= 0.20000\n",
      "Step: 4700, Minibatch Loss= 2.687340, Training Accuracy= 0.13333\n",
      "Step: 4800, Minibatch Loss= 3.869416, Training Accuracy= 0.11667\n",
      "Step: 4900, Minibatch Loss= 8.573560, Training Accuracy= 0.10000\n",
      "Step: 5000, Minibatch Loss= 4.143248, Training Accuracy= 0.10833\n",
      "Step: 5100, Minibatch Loss= 5.238150, Training Accuracy= 0.14167\n",
      "Step: 5200, Minibatch Loss= 8.252874, Training Accuracy= 0.09167\n",
      "Step: 5300, Minibatch Loss= 4.219492, Training Accuracy= 0.09167\n",
      "Step: 5400, Minibatch Loss= 2.698083, Training Accuracy= 0.12500\n",
      "Step: 5500, Minibatch Loss= 4.421133, Training Accuracy= 0.13333\n",
      "Step: 5600, Minibatch Loss= 2.569551, Training Accuracy= 0.07500\n",
      "Step: 5700, Minibatch Loss= 6.438253, Training Accuracy= 0.15000\n",
      "Step: 5800, Minibatch Loss= 5.960334, Training Accuracy= 0.11667\n",
      "Step: 5900, Minibatch Loss= 9.266309, Training Accuracy= 0.14167\n",
      "Optimization Finished!\n",
      "Save model...\n"
     ]
    }
   ],
   "source": [
    "def vgg_train(argv=None):\n",
    "    ##Load Cifar-10 train image and label\n",
    "    X_train_image1 = load_train_data(1)    # load data_batch_1.bin\n",
    "    X_train_image2 = load_train_data(2)    # load data_batch_2.bin\n",
    "    X_train_image3 = load_train_data(3)    # load data_batch_3.bin\n",
    "    X_train_image4 = load_train_data(4)    # load data_batch_4.bin\n",
    "    X_train_image5 = load_train_data(5)    # load data_batch_5.bin\n",
    "    print(X_train_image1.shape)            # (30730000,)\n",
    "\n",
    "    X_train_image=np.concatenate((X_train_image1,X_train_image2,X_train_image3,X_train_image4,X_train_image5),axis=0)\n",
    "    print(X_train_image.shape)             # (153650000,)\n",
    "\n",
    "    # reshape to (50000,3073)\n",
    "    # in one Row ,the 1st byte is the label,other 3072byte =1024 Red +1024 green +1024 blue ch data\n",
    "    X_train_image = X_train_image.reshape(-1, 3073)\n",
    "    tempA = X_train_image.copy()\n",
    "    X_train_image = np.delete(X_train_image, 0, 1)           # delete 1st column data. (obj=0, axis=1)\n",
    "    X_train_image = X_train_image.reshape(-1, 3, 32, 32)     # reshape to (50000,3,32,32)\n",
    "    X_train_image = X_train_image.transpose([0, 2, 3, 1])    # transfer to (50000,32,32,3)\n",
    "    X_train_image = X_train_image.reshape(-1, 3072)          # (50000, 3072)\n",
    "\n",
    "    # split to 3073 col,the first column is the label.\n",
    "    tempA = np.hsplit(tempA, 3073)\n",
    "    X_train_label = np.asarray(tempA[0])\n",
    "    X_train_label = X_train_label.reshape([50000,])         # (50000,)\n",
    "\n",
    "    print(\"X_train_image.shape =\", X_train_image.shape)\n",
    "    X_train_label = encode_labels(X_train_label, 10)\n",
    "    print(\"X_train_lable.shape =\", X_train_label.shape)\n",
    "    # print(X_train_label[0:50])\n",
    "    ##============================\n",
    "\n",
    "    train(X_train_image, X_train_label)\n",
    "\n",
    "vgg_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
