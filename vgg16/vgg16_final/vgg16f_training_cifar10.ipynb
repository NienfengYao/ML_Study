{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic\n",
    "### Step1 \n",
    "* Case1: First, I study from [VGG in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/) and implement it in [vgg16_pretrained_predict.ipynb](../vgg16_pretrained_predict.ipynb).\n",
    "  * Download the pretrained parameters and using the VGG16 model to predict, it works.\n",
    "* Case2: Then, I study from [Ashing00/Tensorflow_VGG](https://github.com/Ashing00/Tensorflow_VGG) and implement it in [vgg_cifar10_train.ipynb](../vgg16_cifar10_train/vgg_cifar10_train.ipynb)\n",
    "  * We using VGG16 modes (but the model has little different with Case1) to do training with CIFAR-10 dataset, it works.\n",
    "* There are some little different in the two VGG16 model \n",
    "  * Case 1 \n",
    "    * Do pre-process in data input, minus the mean value of RGB\n",
    "    * Pooling kernel size = 2x2, stride = 2\n",
    "  * Case 2\n",
    "    * Didn't the pre-process in data input. But it do batch normalization in each hiddle layer.\n",
    "    * Pooling kernel size = 2x2, stride = 2, except The Pool_1 (kernel size = 3x3, stride = 1)\n",
    "* Issue: (Fixed by Step2)\n",
    "  The training is failed, we can't get good accuracy.  \n",
    "  \n",
    "### Step2\n",
    "* I add batch normalization in each hidden layer. Now we can get good accuracy.\n",
    "* Buy Why?\n",
    "  * Before, I think the original VGG16 mode from \"VGG in TensorFlow\" should be good to train with CIFAR-10 dataset, but it doesn't. Did I miss something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from vgg16_cifar10 import vgg16\n",
    "\n",
    "data_dir = \"../../../data/\"\n",
    "extract_folder = 'cifar-10-batches-bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(y, k):\n",
    "    \"\"\"\n",
    "    Encode labels into one-hot representation\n",
    "    y: the items.\n",
    "    k: the class number.\n",
    "    \"\"\"\n",
    "    onehot = np.zeros((y.shape[0], k))\n",
    "    for idx, val in enumerate(y):\n",
    "        onehot[idx,val] = 1.0  ##idx=0~xxxxx，if val =3 ,表示欄位3要設成1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image data from binary files. (train/test)\n",
    "\n",
    "def load_train_data(n):    # n=1,2..5, data_batch_1.bin ~ data_batch_5.bin\n",
    "    \"\"\"Load Cifar10 data from `path`\"\"\"\n",
    "    images_path = os.path.join(data_dir, extract_folder, 'data_batch_{}.bin'.format(n)) \n",
    "    return _load_binary_data(images_path)\n",
    "\n",
    "def load_test_data():      # test_batch\n",
    "    \"\"\"Load Cifar10 test data from `path`\"\"\"\n",
    "    test_path = os.path.join(data_dir, extract_folder, 'test_batch.bin') \n",
    "    return _load_binary_data(test_path)\n",
    "\n",
    "def _load_binary_data(path):\n",
    "    with open(path, 'rb') as img_file:\n",
    "        images = np.fromfile(img_file, dtype=np.uint8)\n",
    "    return images    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "MODEL_SAVE_PATH = \"./vgg/\"\n",
    "MODEL_NAME = \"vgg_cifar_model\"\n",
    "learning_rate = 0.001\n",
    "BATCH_SIZE = 120\n",
    "display_step = 100\n",
    "TRAINING_STEPS = 6000\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3072 # cifar data input (img shape: 32x32x3)\n",
    "n_classes = 10 # cifar10 total classes (0-9 )\n",
    "# Ryan mark\n",
    "# dropout = 0.60 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30730000,)\n",
      "(153650000,)\n",
      "Mean: [125.306918046875, 122.950394140625, 113.86538318359375]\n"
     ]
    }
   ],
   "source": [
    "def get_mean():\n",
    "    X_train_image1 = load_train_data(1)    # load data_batch_1.bin\n",
    "    X_train_image2 = load_train_data(2)    # load data_batch_2.bin\n",
    "    X_train_image3 = load_train_data(3)    # load data_batch_3.bin\n",
    "    X_train_image4 = load_train_data(4)    # load data_batch_4.bin\n",
    "    X_train_image5 = load_train_data(5)    # load data_batch_5.bin\n",
    "    print(X_train_image1.shape)            # (30730000,)\n",
    "\n",
    "    X_train_image=np.concatenate((X_train_image1,X_train_image2,X_train_image3,X_train_image4,X_train_image5),axis=0)\n",
    "    print(X_train_image.shape)             # (153650000,)\n",
    "\n",
    "    # reshape to (50000,3073)\n",
    "    # in one Row ,the 1st byte is the label,other 3072byte =1024 Red +1024 green +1024 blue ch data\n",
    "    X_train_image = X_train_image.reshape(-1, 3073)\n",
    "    tempA = X_train_image.copy()\n",
    "    X_train_image = np.delete(X_train_image, 0, 1)           # delete 1st column data. (obj=0, axis=1)\n",
    "    X_train_image = X_train_image.reshape(-1, 3, 32, 32)     # reshape to (50000,3,32,32)\n",
    "    X_train_image = X_train_image.transpose([0, 2, 3, 1])    # transfer to (50000,32,32,3)\n",
    "    X_train_image = X_train_image.reshape(-1, 3072)          # (50000, 3072)\n",
    "    return list(np.mean(np.reshape(X_train_image, (-1, 32, 32, 3)) , axis=(0, 1, 2)))\n",
    "    \n",
    "MEAN = get_mean()\n",
    "print(\"Mean:\", MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "def train(X_train, y_train_lable, mean):\n",
    "    shuffle = True\n",
    "    batch_idx = 0\n",
    "    batch_len = int( X_train.shape[0] / BATCH_SIZE)\n",
    "    print(\"batch_len=\", batch_len) # 50000/120 = 416.6 => 416\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_idx = np.random.permutation(batch_len)    # 打散資料順序\n",
    "\n",
    "    # tf Graph input\n",
    "    x_ = tf.placeholder(tf.float32, [None, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    # Ryan mark\n",
    "    # keep_prob = tf.placeholder(tf.float32)    # dropout (keep probability)\n",
    "    x = tf.reshape(x_, shape=[-1, 32, 32, 3])\n",
    "\n",
    "    # Construct model\n",
    "    vgg = vgg16(x, mean=mean)\n",
    "    pred = vgg.fc3l\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    #GradientDescentOptimizer\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # 初始化TensorFlow持久化類。\n",
    "    saver = tf.train.Saver()\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        step = 1\n",
    "        print (\"Start  training!\")\n",
    "        # Keep training until reach max iterations:\n",
    "        while step < TRAINING_STEPS:\n",
    "            #batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            if shuffle==True:\n",
    "                batch_shuffle_idx=train_idx[batch_idx]\n",
    "                batch_xs=X_train[batch_shuffle_idx*BATCH_SIZE:batch_shuffle_idx*BATCH_SIZE+BATCH_SIZE]\n",
    "                batch_ys=y_train_lable[batch_shuffle_idx*BATCH_SIZE:batch_shuffle_idx*BATCH_SIZE+BATCH_SIZE]\n",
    "            else:\n",
    "                batch_xs=X_train[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE]\n",
    "                batch_ys=y_train_lable[batch_idx*BATCH_SIZE:batch_idx*BATCH_SIZE+BATCH_SIZE]\n",
    "\n",
    "            if batch_idx<batch_len:\n",
    "                batch_idx+=1\n",
    "                if batch_idx==batch_len:\n",
    "                    batch_idx=0\n",
    "            else:\n",
    "                batch_idx=0\n",
    "            # Ryan: maybe we don't need the reshape? No, We need it, because of the input feature in conv1_1 is 3\n",
    "            reshaped_xs = np.reshape(batch_xs, (BATCH_SIZE, 32, 32, 3))\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(optimizer, feed_dict={x: reshaped_xs, y: batch_ys})\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: reshaped_xs, y: batch_ys})\n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(acc)\n",
    "            if step % display_step == 0:\n",
    "                print(\"Step: \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                    \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                    \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "        print(\"Optimization Finished!\")\n",
    "        print(\"Save model...\")\n",
    "        saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30730000,)\n",
      "(153650000,)\n",
      "X_train_image.shape = (50000, 3072)\n",
      "X_train_lable.shape = (50000, 10)\n",
      "batch_len= 416\n",
      "WARNING:tensorflow:From <ipython-input-6-540c227c3317>:24: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Start  training!\n",
      "Step: 100, Minibatch Loss= 1.616541, Training Accuracy= 0.47500\n",
      "Step: 200, Minibatch Loss= 1.260319, Training Accuracy= 0.55000\n",
      "Step: 300, Minibatch Loss= 1.051058, Training Accuracy= 0.64167\n",
      "Step: 400, Minibatch Loss= 1.136269, Training Accuracy= 0.60833\n",
      "Step: 500, Minibatch Loss= 0.936667, Training Accuracy= 0.61667\n",
      "Step: 600, Minibatch Loss= 0.791617, Training Accuracy= 0.71667\n",
      "Step: 700, Minibatch Loss= 0.825008, Training Accuracy= 0.72500\n",
      "Step: 800, Minibatch Loss= 0.796007, Training Accuracy= 0.71667\n",
      "Step: 900, Minibatch Loss= 0.782628, Training Accuracy= 0.75000\n",
      "Step: 1000, Minibatch Loss= 0.641943, Training Accuracy= 0.80000\n",
      "Step: 1100, Minibatch Loss= 0.991825, Training Accuracy= 0.69167\n",
      "Step: 1200, Minibatch Loss= 0.582827, Training Accuracy= 0.80833\n",
      "Step: 1300, Minibatch Loss= 0.478089, Training Accuracy= 0.84167\n",
      "Step: 1400, Minibatch Loss= 0.425434, Training Accuracy= 0.86667\n",
      "Step: 1500, Minibatch Loss= 0.585701, Training Accuracy= 0.79167\n",
      "Step: 1600, Minibatch Loss= 0.509483, Training Accuracy= 0.84167\n",
      "Step: 1700, Minibatch Loss= 0.247038, Training Accuracy= 0.91667\n",
      "Step: 1800, Minibatch Loss= 0.436678, Training Accuracy= 0.85000\n",
      "Step: 1900, Minibatch Loss= 0.438255, Training Accuracy= 0.85833\n",
      "Step: 2000, Minibatch Loss= 0.393487, Training Accuracy= 0.84167\n",
      "Step: 2100, Minibatch Loss= 0.146630, Training Accuracy= 0.96667\n",
      "Step: 2200, Minibatch Loss= 0.282228, Training Accuracy= 0.90000\n",
      "Step: 2300, Minibatch Loss= 0.169863, Training Accuracy= 0.96667\n",
      "Step: 2400, Minibatch Loss= 0.221328, Training Accuracy= 0.94167\n",
      "Step: 2500, Minibatch Loss= 0.153110, Training Accuracy= 0.97500\n",
      "Step: 2600, Minibatch Loss= 0.150269, Training Accuracy= 0.95833\n",
      "Step: 2700, Minibatch Loss= 0.192522, Training Accuracy= 0.96667\n",
      "Step: 2800, Minibatch Loss= 0.085682, Training Accuracy= 0.98333\n",
      "Step: 2900, Minibatch Loss= 0.076141, Training Accuracy= 0.99167\n",
      "Step: 3000, Minibatch Loss= 0.146115, Training Accuracy= 0.98333\n",
      "Step: 3100, Minibatch Loss= 0.070485, Training Accuracy= 0.98333\n",
      "Step: 3200, Minibatch Loss= 0.116539, Training Accuracy= 0.97500\n",
      "Step: 3300, Minibatch Loss= 0.082371, Training Accuracy= 0.99167\n",
      "Step: 3400, Minibatch Loss= 0.031850, Training Accuracy= 1.00000\n",
      "Step: 3500, Minibatch Loss= 0.055616, Training Accuracy= 1.00000\n",
      "Step: 3600, Minibatch Loss= 0.084319, Training Accuracy= 0.97500\n",
      "Step: 3700, Minibatch Loss= 0.041468, Training Accuracy= 1.00000\n",
      "Step: 3800, Minibatch Loss= 0.050665, Training Accuracy= 0.99167\n",
      "Step: 3900, Minibatch Loss= 0.044624, Training Accuracy= 0.99167\n",
      "Step: 4000, Minibatch Loss= 0.013747, Training Accuracy= 1.00000\n",
      "Step: 4100, Minibatch Loss= 0.035796, Training Accuracy= 0.99167\n",
      "Step: 4200, Minibatch Loss= 0.037539, Training Accuracy= 0.99167\n",
      "Step: 4300, Minibatch Loss= 0.066967, Training Accuracy= 0.99167\n",
      "Step: 4400, Minibatch Loss= 0.019752, Training Accuracy= 1.00000\n",
      "Step: 4500, Minibatch Loss= 0.067851, Training Accuracy= 0.99167\n",
      "Step: 4600, Minibatch Loss= 0.027745, Training Accuracy= 0.98333\n",
      "Step: 4700, Minibatch Loss= 0.046014, Training Accuracy= 0.99167\n",
      "Step: 4800, Minibatch Loss= 0.024333, Training Accuracy= 1.00000\n",
      "Step: 4900, Minibatch Loss= 0.267714, Training Accuracy= 0.95000\n",
      "Step: 5000, Minibatch Loss= 0.018579, Training Accuracy= 1.00000\n",
      "Step: 5100, Minibatch Loss= 0.046724, Training Accuracy= 0.99167\n",
      "Step: 5200, Minibatch Loss= 0.033285, Training Accuracy= 0.99167\n",
      "Step: 5300, Minibatch Loss= 0.021013, Training Accuracy= 1.00000\n",
      "Step: 5400, Minibatch Loss= 0.019383, Training Accuracy= 0.99167\n",
      "Step: 5500, Minibatch Loss= 0.027381, Training Accuracy= 0.99167\n",
      "Step: 5600, Minibatch Loss= 0.009866, Training Accuracy= 1.00000\n",
      "Step: 5700, Minibatch Loss= 0.009556, Training Accuracy= 1.00000\n",
      "Step: 5800, Minibatch Loss= 0.020193, Training Accuracy= 1.00000\n",
      "Step: 5900, Minibatch Loss= 0.023791, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Save model...\n"
     ]
    }
   ],
   "source": [
    "def vgg_train(mean, argv=None):\n",
    "    ##Load Cifar-10 train image and label\n",
    "    X_train_image1 = load_train_data(1)    # load data_batch_1.bin\n",
    "    X_train_image2 = load_train_data(2)    # load data_batch_2.bin\n",
    "    X_train_image3 = load_train_data(3)    # load data_batch_3.bin\n",
    "    X_train_image4 = load_train_data(4)    # load data_batch_4.bin\n",
    "    X_train_image5 = load_train_data(5)    # load data_batch_5.bin\n",
    "    print(X_train_image1.shape)            # (30730000,)\n",
    "\n",
    "    X_train_image=np.concatenate((X_train_image1,X_train_image2,X_train_image3,X_train_image4,X_train_image5),axis=0)\n",
    "    print(X_train_image.shape)             # (153650000,)\n",
    "\n",
    "    # reshape to (50000,3073)\n",
    "    # in one Row ,the 1st byte is the label,other 3072byte =1024 Red +1024 green +1024 blue ch data\n",
    "    X_train_image = X_train_image.reshape(-1, 3073)\n",
    "    tempA = X_train_image.copy()\n",
    "    X_train_image = np.delete(X_train_image, 0, 1)           # delete 1st column data. (obj=0, axis=1)\n",
    "    X_train_image = X_train_image.reshape(-1, 3, 32, 32)     # reshape to (50000,3,32,32)\n",
    "    X_train_image = X_train_image.transpose([0, 2, 3, 1])    # transfer to (50000,32,32,3)\n",
    "    X_train_image = X_train_image.reshape(-1, 3072)          # (50000, 3072)\n",
    "\n",
    "    # split to 3073 col,the first column is the label.\n",
    "    tempA = np.hsplit(tempA, 3073)\n",
    "    X_train_label = np.asarray(tempA[0])\n",
    "    X_train_label = X_train_label.reshape([50000,])         # (50000,)\n",
    "\n",
    "    print(\"X_train_image.shape =\", X_train_image.shape)\n",
    "    X_train_label = encode_labels(X_train_label, 10)\n",
    "    print(\"X_train_lable.shape =\", X_train_label.shape)\n",
    "    # print(X_train_label[0:50])\n",
    "    ##============================\n",
    "\n",
    "    train(X_train_image, X_train_label, mean)\n",
    "\n",
    "vgg_train(MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate with full test dataset\n",
    "上例的 accuracy 是由 train dataset 計算出來的，不客觀。\n",
    "以下則是以 test dataset 為 input 計算其 accuracy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_image.shape= (10000, 3072)\n",
      "X_test_label.shape= (10000, 10)\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "INFO:tensorflow:Restoring parameters from ./vgg/vgg_cifar_model\n",
      "Test  batch  0 :Testing Accuracy: 0.78333336\n",
      "Test  batch  1 :Testing Accuracy: 0.825\n",
      "Test  batch  2 :Testing Accuracy: 0.78333336\n",
      "Test  batch  3 :Testing Accuracy: 0.7416667\n",
      "Test  batch  4 :Testing Accuracy: 0.8666667\n",
      "Test  batch  5 :Testing Accuracy: 0.85833335\n",
      "Test  batch  6 :Testing Accuracy: 0.775\n",
      "Test  batch  7 :Testing Accuracy: 0.825\n",
      "Test  batch  8 :Testing Accuracy: 0.7583333\n",
      "Test  batch  9 :Testing Accuracy: 0.81666666\n",
      "Test  batch  10 :Testing Accuracy: 0.7583333\n",
      "Test  batch  11 :Testing Accuracy: 0.8\n",
      "Test  batch  12 :Testing Accuracy: 0.71666664\n",
      "Test  batch  13 :Testing Accuracy: 0.78333336\n",
      "Test  batch  14 :Testing Accuracy: 0.71666664\n",
      "Test  batch  15 :Testing Accuracy: 0.7583333\n",
      "Test  batch  16 :Testing Accuracy: 0.6666667\n",
      "Test  batch  17 :Testing Accuracy: 0.80833334\n",
      "Test  batch  18 :Testing Accuracy: 0.81666666\n",
      "Test  batch  19 :Testing Accuracy: 0.78333336\n",
      "Test  batch  20 :Testing Accuracy: 0.75\n",
      "Test  batch  21 :Testing Accuracy: 0.69166666\n",
      "Test  batch  22 :Testing Accuracy: 0.8333333\n",
      "Test  batch  23 :Testing Accuracy: 0.78333336\n",
      "Test  batch  24 :Testing Accuracy: 0.76666665\n",
      "Test  batch  25 :Testing Accuracy: 0.75\n",
      "Test  batch  26 :Testing Accuracy: 0.78333336\n",
      "Test  batch  27 :Testing Accuracy: 0.7416667\n",
      "Test  batch  28 :Testing Accuracy: 0.7416667\n",
      "Test  batch  29 :Testing Accuracy: 0.825\n",
      "Test  batch  30 :Testing Accuracy: 0.7916667\n",
      "Test  batch  31 :Testing Accuracy: 0.75\n",
      "Test  batch  32 :Testing Accuracy: 0.8333333\n",
      "Test  batch  33 :Testing Accuracy: 0.80833334\n",
      "Test  batch  34 :Testing Accuracy: 0.8\n",
      "Test  batch  35 :Testing Accuracy: 0.7916667\n",
      "Test  batch  36 :Testing Accuracy: 0.8333333\n",
      "Test  batch  37 :Testing Accuracy: 0.76666665\n",
      "Test  batch  38 :Testing Accuracy: 0.7916667\n",
      "Test  batch  39 :Testing Accuracy: 0.81666666\n",
      "Test  batch  40 :Testing Accuracy: 0.825\n",
      "Test  batch  41 :Testing Accuracy: 0.7083333\n",
      "Test  batch  42 :Testing Accuracy: 0.8\n",
      "Test  batch  43 :Testing Accuracy: 0.75\n",
      "Test  batch  44 :Testing Accuracy: 0.825\n",
      "Test  batch  45 :Testing Accuracy: 0.73333335\n",
      "Test  batch  46 :Testing Accuracy: 0.7583333\n",
      "Test  batch  47 :Testing Accuracy: 0.775\n",
      "Test  batch  48 :Testing Accuracy: 0.78333336\n",
      "Test  batch  49 :Testing Accuracy: 0.76666665\n",
      "Test  batch  50 :Testing Accuracy: 0.825\n",
      "Test  batch  51 :Testing Accuracy: 0.7416667\n",
      "Test  batch  52 :Testing Accuracy: 0.8333333\n",
      "Test  batch  53 :Testing Accuracy: 0.7\n",
      "Test  batch  54 :Testing Accuracy: 0.80833334\n",
      "Test  batch  55 :Testing Accuracy: 0.81666666\n",
      "Test  batch  56 :Testing Accuracy: 0.75\n",
      "Test  batch  57 :Testing Accuracy: 0.7416667\n",
      "Test  batch  58 :Testing Accuracy: 0.725\n",
      "Test  batch  59 :Testing Accuracy: 0.7583333\n",
      "Test  batch  60 :Testing Accuracy: 0.825\n",
      "Test  batch  61 :Testing Accuracy: 0.8\n",
      "Test  batch  62 :Testing Accuracy: 0.825\n",
      "Test  batch  63 :Testing Accuracy: 0.8\n",
      "Test  batch  64 :Testing Accuracy: 0.80833334\n",
      "Test  batch  65 :Testing Accuracy: 0.7583333\n",
      "Test  batch  66 :Testing Accuracy: 0.8666667\n",
      "Test  batch  67 :Testing Accuracy: 0.7916667\n",
      "Test  batch  68 :Testing Accuracy: 0.675\n",
      "Test  batch  69 :Testing Accuracy: 0.73333335\n",
      "Test  batch  70 :Testing Accuracy: 0.7916667\n",
      "Test  batch  71 :Testing Accuracy: 0.81666666\n",
      "Test  batch  72 :Testing Accuracy: 0.775\n",
      "Test  batch  73 :Testing Accuracy: 0.7916667\n",
      "Test  batch  74 :Testing Accuracy: 0.76666665\n",
      "Test  batch  75 :Testing Accuracy: 0.81666666\n",
      "Test  batch  76 :Testing Accuracy: 0.80833334\n",
      "Test  batch  77 :Testing Accuracy: 0.80833334\n",
      "Test  batch  78 :Testing Accuracy: 0.7416667\n",
      "Test  batch  79 :Testing Accuracy: 0.825\n",
      "Test  batch  80 :Testing Accuracy: 0.84166664\n",
      "Test  batch  81 :Testing Accuracy: 0.73333335\n",
      "Test  batch  82 :Testing Accuracy: 0.7583333\n",
      "Average Testing Accuracy= 0.7816265\n"
     ]
    }
   ],
   "source": [
    "def evaluate(X_test,y_test_lable, mean):\n",
    "    with tf.Graph().as_default() as g:\n",
    "\n",
    "        # 定義輸出為4維矩陣的 placeholder\n",
    "        x_ = tf.placeholder(tf.float32, [None, n_input])\n",
    "        x = tf.reshape(x_, shape=[-1, 32, 32, 3])\n",
    "        y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "        # Construct model\n",
    "        vgg = vgg16(x, mean)\n",
    "        pred = vgg.fc3l\n",
    "\n",
    "        # Evaluate model\n",
    "        correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        test_batch_len =int( X_test.shape[0]/BATCH_SIZE)\n",
    "        test_acc=[]\n",
    "\n",
    "        test_xs = np.reshape(X_test, (X_test.shape[0], 32, 32, 3))\n",
    "        batchsize = BATCH_SIZE\n",
    "\n",
    "        # 'Saver' op to save and restore all the variables\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess,\"./vgg/vgg_cifar_model\")\n",
    "\n",
    "            for i in range(test_batch_len):\n",
    "                temp_acc= sess.run(accuracy, feed_dict={x: test_xs[batchsize*i:batchsize*i+batchsize], y: y_test_lable[batchsize*i:batchsize*i+batchsize]})\n",
    "                test_acc.append(temp_acc)\n",
    "                print (\"Test  batch \",i,\":Testing Accuracy:\",temp_acc)\n",
    "\n",
    "            t_acc=tf.reduce_mean(tf.cast(test_acc, tf.float32))\n",
    "            print(\"Average Testing Accuracy=\",sess.run(t_acc))\n",
    "            return\n",
    "\n",
    "        \n",
    "def vgg_eval(mean, argv=None):\n",
    "    ##Load Cifar-10 test image  and label\t\n",
    "    X_test_image = load_test_data()\t#load test_batch.bin\n",
    "    #reshape to (10000,3073)\n",
    "    #in one Row ,the 1st byte is the label,other 3072byte =1024 Red +1024 green +1024 blue ch data\n",
    "    X_test_image=X_test_image.reshape(-1,3073)\n",
    "    tempA=X_test_image.copy()\n",
    "    X_test_image=np.delete(X_test_image, 0, 1) #delete 1st column data\n",
    "    X_test_image=X_test_image.reshape(-1,3,32,32)  #(1000,3,32,32)\n",
    "    X_test_image = X_test_image.transpose([0, 2, 3, 1])\t#transfer to (10000,32,32,3)\n",
    "    X_test_image=X_test_image.reshape(-1,3072)  #(50000,3,32,32)\n",
    "\n",
    "    #split to 3073 col,the first column is the label.\n",
    "    tempA=np.hsplit(tempA,3073)\t\n",
    "    X_test_label=np.asarray(tempA[0])\n",
    "    X_test_label=X_test_label.reshape([10000,])\n",
    "\n",
    "\n",
    "    #mms=MinMaxScaler()\n",
    "    #X_test_image=mms.fit_transform(X_test_image)\n",
    "\n",
    "    X_test_label = encode_labels(X_test_label,10)\n",
    "    print(\"X_test_image.shape=\",X_test_image.shape)\n",
    "    print(\"X_test_label.shape=\",X_test_label.shape)\n",
    "    print(X_test_label[0:5])\n",
    "    evaluate(X_test_image,X_test_label, mean)\n",
    "\n",
    "vgg_eval(mean=MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
