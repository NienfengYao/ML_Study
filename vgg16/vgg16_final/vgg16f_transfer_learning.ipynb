{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Transfer Learning\n",
    "The source is from [agoila/transfer-learning](https://github.com/agoila/transfer-learning), I also implement it locally [Transfer_Learning.ipynb](../transfer-learning/Transfer_Learning.ipynb)\n",
    "\n",
    "Now, I will redo it but using the VGG16 model from [VGG in TensorFlow](https://www.cs.toronto.edu/~frossard/post/vgg16/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from vgg16_transfer_learn import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['daisy', 'sunflowers', 'roses', 'dandelion', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../transfer-learning/flower_photos/'\n",
    "contents = os.listdir(data_dir)\n",
    "classes = [each for each in contents if os.path.isdir(data_dir + each)]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns image of shape [224, 224, 3]\n",
    "# [height, width, depth]\n",
    "def load_image(path):\n",
    "    # load image\n",
    "    img = skimage.io.imread(path)\n",
    "    img = img / 255.0\n",
    "    assert (0 <= img).all() and (img <= 1.0).all()\n",
    "    # print(\"Original Image Shape:\", img.shape)\n",
    "    # we crop image from center\n",
    "    short_edge = min(img.shape[:2])\n",
    "    # print(\"short_edge:\", short_edge)\n",
    "    yy = int((img.shape[0] - short_edge) / 2)\n",
    "    xx = int((img.shape[1] - short_edge) / 2)\n",
    "    # print(yy, xx)\n",
    "    crop_img = img[yy: yy + short_edge, xx: xx + short_edge]\n",
    "    # resize to 224, 224\n",
    "    resized_img = skimage.transform.resize(crop_img, (224, 224), mode='constant')\n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_path = data_dir + classes[0]\n",
    "# files = os.listdir(class_path)\n",
    "# img = load_image(os.path.join(class_path, files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model started\n",
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "26 fc6_W (25088, 4096)\n",
      "27 fc6_b (4096,)\n",
      "28 fc7_W (4096, 4096)\n",
      "29 fc7_b (4096,)\n",
      "30 fc8_W (4096, 1000)\n",
      "31 fc8_b (1000,)\n",
      "build model finished: 11s\n",
      "Starting daisy images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanyao/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n",
      "330 images processed\n",
      "340 images processed\n",
      "350 images processed\n",
      "360 images processed\n",
      "370 images processed\n",
      "380 images processed\n",
      "390 images processed\n",
      "400 images processed\n",
      "410 images processed\n",
      "420 images processed\n",
      "430 images processed\n",
      "440 images processed\n",
      "450 images processed\n",
      "460 images processed\n",
      "470 images processed\n",
      "480 images processed\n",
      "490 images processed\n",
      "500 images processed\n",
      "510 images processed\n",
      "520 images processed\n",
      "530 images processed\n",
      "540 images processed\n",
      "550 images processed\n",
      "560 images processed\n",
      "570 images processed\n",
      "580 images processed\n",
      "590 images processed\n",
      "600 images processed\n",
      "610 images processed\n",
      "620 images processed\n",
      "630 images processed\n",
      "633 images processed\n",
      "Starting sunflowers images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n",
      "330 images processed\n",
      "340 images processed\n",
      "350 images processed\n",
      "360 images processed\n",
      "370 images processed\n",
      "380 images processed\n",
      "390 images processed\n",
      "400 images processed\n",
      "410 images processed\n",
      "420 images processed\n",
      "430 images processed\n",
      "440 images processed\n",
      "450 images processed\n",
      "460 images processed\n",
      "470 images processed\n",
      "480 images processed\n",
      "490 images processed\n",
      "500 images processed\n",
      "510 images processed\n",
      "520 images processed\n",
      "530 images processed\n",
      "540 images processed\n",
      "550 images processed\n",
      "560 images processed\n",
      "570 images processed\n",
      "580 images processed\n",
      "590 images processed\n",
      "600 images processed\n",
      "610 images processed\n",
      "620 images processed\n",
      "630 images processed\n",
      "640 images processed\n",
      "650 images processed\n",
      "660 images processed\n",
      "670 images processed\n",
      "680 images processed\n",
      "690 images processed\n",
      "699 images processed\n",
      "Starting roses images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n",
      "330 images processed\n",
      "340 images processed\n",
      "350 images processed\n",
      "360 images processed\n",
      "370 images processed\n",
      "380 images processed\n",
      "390 images processed\n",
      "400 images processed\n",
      "410 images processed\n",
      "420 images processed\n",
      "430 images processed\n",
      "440 images processed\n",
      "450 images processed\n",
      "460 images processed\n",
      "470 images processed\n",
      "480 images processed\n",
      "490 images processed\n",
      "500 images processed\n",
      "510 images processed\n",
      "520 images processed\n",
      "530 images processed\n",
      "540 images processed\n",
      "550 images processed\n",
      "560 images processed\n",
      "570 images processed\n",
      "580 images processed\n",
      "590 images processed\n",
      "600 images processed\n",
      "610 images processed\n",
      "620 images processed\n",
      "630 images processed\n",
      "640 images processed\n",
      "641 images processed\n",
      "Starting dandelion images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n",
      "330 images processed\n",
      "340 images processed\n",
      "350 images processed\n",
      "360 images processed\n",
      "370 images processed\n",
      "380 images processed\n",
      "390 images processed\n",
      "400 images processed\n",
      "410 images processed\n",
      "420 images processed\n",
      "430 images processed\n",
      "440 images processed\n",
      "450 images processed\n",
      "460 images processed\n",
      "470 images processed\n",
      "480 images processed\n",
      "490 images processed\n",
      "500 images processed\n",
      "510 images processed\n",
      "520 images processed\n",
      "530 images processed\n",
      "540 images processed\n",
      "550 images processed\n",
      "560 images processed\n",
      "570 images processed\n",
      "580 images processed\n",
      "590 images processed\n",
      "600 images processed\n",
      "610 images processed\n",
      "620 images processed\n",
      "630 images processed\n",
      "640 images processed\n",
      "650 images processed\n",
      "660 images processed\n",
      "670 images processed\n",
      "680 images processed\n",
      "690 images processed\n",
      "700 images processed\n",
      "710 images processed\n",
      "720 images processed\n",
      "730 images processed\n",
      "740 images processed\n",
      "750 images processed\n",
      "760 images processed\n",
      "770 images processed\n",
      "780 images processed\n",
      "790 images processed\n",
      "800 images processed\n",
      "810 images processed\n",
      "820 images processed\n",
      "830 images processed\n",
      "840 images processed\n",
      "850 images processed\n",
      "860 images processed\n",
      "870 images processed\n",
      "880 images processed\n",
      "890 images processed\n",
      "898 images processed\n",
      "Starting tulips images\n",
      "10 images processed\n",
      "20 images processed\n",
      "30 images processed\n",
      "40 images processed\n",
      "50 images processed\n",
      "60 images processed\n",
      "70 images processed\n",
      "80 images processed\n",
      "90 images processed\n",
      "100 images processed\n",
      "110 images processed\n",
      "120 images processed\n",
      "130 images processed\n",
      "140 images processed\n",
      "150 images processed\n",
      "160 images processed\n",
      "170 images processed\n",
      "180 images processed\n",
      "190 images processed\n",
      "200 images processed\n",
      "210 images processed\n",
      "220 images processed\n",
      "230 images processed\n",
      "240 images processed\n",
      "250 images processed\n",
      "260 images processed\n",
      "270 images processed\n",
      "280 images processed\n",
      "290 images processed\n",
      "300 images processed\n",
      "310 images processed\n",
      "320 images processed\n",
      "330 images processed\n",
      "340 images processed\n",
      "350 images processed\n",
      "360 images processed\n",
      "370 images processed\n",
      "380 images processed\n",
      "390 images processed\n",
      "400 images processed\n",
      "410 images processed\n",
      "420 images processed\n",
      "430 images processed\n",
      "440 images processed\n",
      "450 images processed\n",
      "460 images processed\n",
      "470 images processed\n",
      "480 images processed\n",
      "490 images processed\n",
      "500 images processed\n",
      "510 images processed\n",
      "520 images processed\n",
      "530 images processed\n",
      "540 images processed\n",
      "550 images processed\n",
      "560 images processed\n",
      "570 images processed\n",
      "580 images processed\n",
      "590 images processed\n",
      "600 images processed\n",
      "610 images processed\n",
      "620 images processed\n",
      "630 images processed\n",
      "640 images processed\n",
      "650 images processed\n",
      "660 images processed\n",
      "670 images processed\n",
      "680 images processed\n",
      "690 images processed\n",
      "700 images processed\n",
      "710 images processed\n",
      "720 images processed\n",
      "730 images processed\n",
      "740 images processed\n",
      "750 images processed\n",
      "760 images processed\n",
      "770 images processed\n",
      "780 images processed\n",
      "790 images processed\n",
      "799 images processed\n"
     ]
    }
   ],
   "source": [
    "# Set the batch size higher if you can fit in in your GPU memory\n",
    "batch_size = 10\n",
    "codes_list = []\n",
    "labels = []\n",
    "batch = []\n",
    "\n",
    "codes = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # TODO: Build the vgg network here\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    vgg = vgg16(input_, 'vgg16_weights.npz', sess)\n",
    "\n",
    "    for each in classes:\n",
    "        print(\"Starting {} images\".format(each))\n",
    "        class_path = data_dir + each\n",
    "        files = os.listdir(class_path)\n",
    "        for ii, file in enumerate(files, 1):\n",
    "            # Add images to the current batch\n",
    "            # utils.load_image crops the input images for us, from the center\n",
    "            img = load_image(os.path.join(class_path, file))\n",
    "            batch.append(img.reshape((1, 224, 224, 3)))\n",
    "            labels.append(each)\n",
    "            \n",
    "            # Running the batch through the network to get the codes\n",
    "            if ii % batch_size == 0 or ii == len(files):\n",
    "                \n",
    "                # Image batch to pass to VGG network\n",
    "                images = np.concatenate(batch)\n",
    "                \n",
    "                # TODO: Get the values from the relu6 layer of the VGG network\n",
    "                feed_dict = {input_: images}\n",
    "                codes_batch = sess.run(vgg.fc1, feed_dict=feed_dict)\n",
    "                \n",
    "                # Here I'm building an array of the codes\n",
    "                if codes is None:\n",
    "                    codes = codes_batch\n",
    "                else:\n",
    "                    codes = np.concatenate((codes, codes_batch))\n",
    "                \n",
    "                # Reset to start building the next batch\n",
    "                batch = []\n",
    "                print('{} images processed'.format(ii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write codes to file\n",
    "with open('codes', 'w') as f:\n",
    "    codes.tofile(f)\n",
    "    \n",
    "# write labels to file\n",
    "import csv\n",
    "with open('labels', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter='\\n')\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Classifier\n",
    "Now that we have codes for all the images, we can build a simple classifier on top of them. The codes behave just like normal input into a simple neural network. Below I'm going to have you do most of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read codes and labels from file\n",
    "import csv\n",
    "\n",
    "with open('labels') as f:\n",
    "    reader = csv.reader(f, delimiter='\\n')\n",
    "    labels = np.array([each for each in reader if len(each) > 0]).squeeze()\n",
    "with open('codes') as f:\n",
    "    codes = np.fromfile(f, dtype=np.float32)\n",
    "    codes = codes.reshape((len(labels), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "labels_vecs = lb.fit_transform(labels) # Your one-hot encoded labels array here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "train_idx, val_idx = next(sss.split(codes, labels_vecs))\n",
    "validSplitSize = int(len(val_idx)/2)\n",
    "\n",
    "val_idx, test_idx = val_idx[:validSplitSize], val_idx[validSplitSize:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y = codes[test_idx], labels_vecs[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes (x, y): (2936, 4096) (2936, 5)\n",
      "Validation shapes (x, y): (367, 4096) (367, 5)\n",
      "Test shapes (x, y): (367, 4096) (367, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-dc6bb3d4be50>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs_ = tf.placeholder(tf.float32, shape=[None, codes.shape[1]])\n",
    "labels_ = tf.placeholder(tf.int64, shape=[None, labels_vecs.shape[1]])\n",
    "\n",
    "# TODO: Classifier layers and operations\n",
    "fc = tf.contrib.layers.fully_connected(inputs_, 512)\n",
    "\n",
    "logits = tf.contrib.layers.fully_connected(fc, labels_vecs.shape[1], activation_fn=None)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels_, logits=logits)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Operations for validation/test accuracy\n",
    "predicted = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, n_batches=10):\n",
    "    \"\"\" Return a generator that yields batches from arrays x and y. \"\"\"\n",
    "    batch_size = len(x)//n_batches\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # If we're not on the last batch, grab data with size batch_size\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # On the last batch, grab the rest of the data\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        # I love generators\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Iteration: 0 Training Loss: 8.13290\n",
      "Epoch: 1/10 Iteration: 1 Training Loss: 27.75088\n",
      "Epoch: 1/10 Iteration: 2 Training Loss: 30.47755\n",
      "Epoch: 1/10 Iteration: 3 Training Loss: 10.46250\n",
      "Epoch: 1/10 Iteration: 4 Training Loss: 4.56607\n",
      "Epoch: 1/10 Iteration: 5 Validation accuracy: 0.62943\n",
      "Epoch: 1/10 Iteration: 5 Training Loss: 5.14031\n",
      "Epoch: 1/10 Iteration: 6 Training Loss: 6.91324\n",
      "Epoch: 1/10 Iteration: 7 Training Loss: 7.15179\n",
      "Epoch: 1/10 Iteration: 8 Training Loss: 6.94140\n",
      "Epoch: 1/10 Iteration: 9 Training Loss: 5.46307\n",
      "Epoch: 1/10 Iteration: 10 Validation accuracy: 0.71117\n",
      "Epoch: 2/10 Iteration: 10 Training Loss: 4.29187\n",
      "Epoch: 2/10 Iteration: 11 Training Loss: 2.07225\n",
      "Epoch: 2/10 Iteration: 12 Training Loss: 1.65150\n",
      "Epoch: 2/10 Iteration: 13 Training Loss: 1.20557\n",
      "Epoch: 2/10 Iteration: 14 Training Loss: 0.95366\n",
      "Epoch: 2/10 Iteration: 15 Validation accuracy: 0.77929\n",
      "Epoch: 2/10 Iteration: 15 Training Loss: 1.17806\n",
      "Epoch: 2/10 Iteration: 16 Training Loss: 1.18930\n",
      "Epoch: 2/10 Iteration: 17 Training Loss: 1.07694\n",
      "Epoch: 2/10 Iteration: 18 Training Loss: 0.91382\n",
      "Epoch: 2/10 Iteration: 19 Training Loss: 1.32038\n",
      "Epoch: 2/10 Iteration: 20 Validation accuracy: 0.77112\n",
      "Epoch: 3/10 Iteration: 20 Training Loss: 1.18431\n",
      "Epoch: 3/10 Iteration: 21 Training Loss: 0.72557\n",
      "Epoch: 3/10 Iteration: 22 Training Loss: 0.47738\n",
      "Epoch: 3/10 Iteration: 23 Training Loss: 0.56667\n",
      "Epoch: 3/10 Iteration: 24 Training Loss: 0.64983\n",
      "Epoch: 3/10 Iteration: 25 Validation accuracy: 0.76839\n",
      "Epoch: 3/10 Iteration: 25 Training Loss: 0.65728\n",
      "Epoch: 3/10 Iteration: 26 Training Loss: 0.76001\n",
      "Epoch: 3/10 Iteration: 27 Training Loss: 0.49645\n",
      "Epoch: 3/10 Iteration: 28 Training Loss: 0.44627\n",
      "Epoch: 3/10 Iteration: 29 Training Loss: 0.48567\n",
      "Epoch: 3/10 Iteration: 30 Validation accuracy: 0.82561\n",
      "Epoch: 4/10 Iteration: 30 Training Loss: 0.46328\n",
      "Epoch: 4/10 Iteration: 31 Training Loss: 0.32182\n",
      "Epoch: 4/10 Iteration: 32 Training Loss: 0.27883\n",
      "Epoch: 4/10 Iteration: 33 Training Loss: 0.22884\n",
      "Epoch: 4/10 Iteration: 34 Training Loss: 0.31755\n",
      "Epoch: 4/10 Iteration: 35 Validation accuracy: 0.80654\n",
      "Epoch: 4/10 Iteration: 35 Training Loss: 0.35830\n",
      "Epoch: 4/10 Iteration: 36 Training Loss: 0.37496\n",
      "Epoch: 4/10 Iteration: 37 Training Loss: 0.28696\n",
      "Epoch: 4/10 Iteration: 38 Training Loss: 0.24719\n",
      "Epoch: 4/10 Iteration: 39 Training Loss: 0.37146\n",
      "Epoch: 4/10 Iteration: 40 Validation accuracy: 0.81199\n",
      "Epoch: 5/10 Iteration: 40 Training Loss: 0.34722\n",
      "Epoch: 5/10 Iteration: 41 Training Loss: 0.23692\n",
      "Epoch: 5/10 Iteration: 42 Training Loss: 0.17108\n",
      "Epoch: 5/10 Iteration: 43 Training Loss: 0.20428\n",
      "Epoch: 5/10 Iteration: 44 Training Loss: 0.19660\n",
      "Epoch: 5/10 Iteration: 45 Validation accuracy: 0.82289\n",
      "Epoch: 5/10 Iteration: 45 Training Loss: 0.24705\n",
      "Epoch: 5/10 Iteration: 46 Training Loss: 0.25209\n",
      "Epoch: 5/10 Iteration: 47 Training Loss: 0.19314\n",
      "Epoch: 5/10 Iteration: 48 Training Loss: 0.17448\n",
      "Epoch: 5/10 Iteration: 49 Training Loss: 0.23802\n",
      "Epoch: 5/10 Iteration: 50 Validation accuracy: 0.82561\n",
      "Epoch: 6/10 Iteration: 50 Training Loss: 0.20754\n",
      "Epoch: 6/10 Iteration: 51 Training Loss: 0.15211\n",
      "Epoch: 6/10 Iteration: 52 Training Loss: 0.11739\n",
      "Epoch: 6/10 Iteration: 53 Training Loss: 0.11629\n",
      "Epoch: 6/10 Iteration: 54 Training Loss: 0.13731\n",
      "Epoch: 6/10 Iteration: 55 Validation accuracy: 0.84469\n",
      "Epoch: 6/10 Iteration: 55 Training Loss: 0.19010\n",
      "Epoch: 6/10 Iteration: 56 Training Loss: 0.19177\n",
      "Epoch: 6/10 Iteration: 57 Training Loss: 0.13008\n",
      "Epoch: 6/10 Iteration: 58 Training Loss: 0.10844\n",
      "Epoch: 6/10 Iteration: 59 Training Loss: 0.17160\n",
      "Epoch: 6/10 Iteration: 60 Validation accuracy: 0.85014\n",
      "Epoch: 7/10 Iteration: 60 Training Loss: 0.16018\n",
      "Epoch: 7/10 Iteration: 61 Training Loss: 0.11860\n",
      "Epoch: 7/10 Iteration: 62 Training Loss: 0.08770\n",
      "Epoch: 7/10 Iteration: 63 Training Loss: 0.08477\n",
      "Epoch: 7/10 Iteration: 64 Training Loss: 0.09473\n",
      "Epoch: 7/10 Iteration: 65 Validation accuracy: 0.85286\n",
      "Epoch: 7/10 Iteration: 65 Training Loss: 0.14158\n",
      "Epoch: 7/10 Iteration: 66 Training Loss: 0.13522\n",
      "Epoch: 7/10 Iteration: 67 Training Loss: 0.08723\n",
      "Epoch: 7/10 Iteration: 68 Training Loss: 0.09621\n",
      "Epoch: 7/10 Iteration: 69 Training Loss: 0.12345\n",
      "Epoch: 7/10 Iteration: 70 Validation accuracy: 0.85286\n",
      "Epoch: 8/10 Iteration: 70 Training Loss: 0.11623\n",
      "Epoch: 8/10 Iteration: 71 Training Loss: 0.08643\n",
      "Epoch: 8/10 Iteration: 72 Training Loss: 0.05751\n",
      "Epoch: 8/10 Iteration: 73 Training Loss: 0.05383\n",
      "Epoch: 8/10 Iteration: 74 Training Loss: 0.06969\n",
      "Epoch: 8/10 Iteration: 75 Validation accuracy: 0.85014\n",
      "Epoch: 8/10 Iteration: 75 Training Loss: 0.10691\n",
      "Epoch: 8/10 Iteration: 76 Training Loss: 0.09728\n",
      "Epoch: 8/10 Iteration: 77 Training Loss: 0.06881\n",
      "Epoch: 8/10 Iteration: 78 Training Loss: 0.06746\n",
      "Epoch: 8/10 Iteration: 79 Training Loss: 0.09712\n",
      "Epoch: 8/10 Iteration: 80 Validation accuracy: 0.85014\n",
      "Epoch: 9/10 Iteration: 80 Training Loss: 0.09279\n",
      "Epoch: 9/10 Iteration: 81 Training Loss: 0.06797\n",
      "Epoch: 9/10 Iteration: 82 Training Loss: 0.04341\n",
      "Epoch: 9/10 Iteration: 83 Training Loss: 0.04142\n",
      "Epoch: 9/10 Iteration: 84 Training Loss: 0.04910\n",
      "Epoch: 9/10 Iteration: 85 Validation accuracy: 0.85014\n",
      "Epoch: 9/10 Iteration: 85 Training Loss: 0.08276\n",
      "Epoch: 9/10 Iteration: 86 Training Loss: 0.06839\n",
      "Epoch: 9/10 Iteration: 87 Training Loss: 0.05357\n",
      "Epoch: 9/10 Iteration: 88 Training Loss: 0.04752\n",
      "Epoch: 9/10 Iteration: 89 Training Loss: 0.07319\n",
      "Epoch: 9/10 Iteration: 90 Validation accuracy: 0.85559\n",
      "Epoch: 10/10 Iteration: 90 Training Loss: 0.07241\n",
      "Epoch: 10/10 Iteration: 91 Training Loss: 0.05213\n",
      "Epoch: 10/10 Iteration: 92 Training Loss: 0.03378\n",
      "Epoch: 10/10 Iteration: 93 Training Loss: 0.02822\n",
      "Epoch: 10/10 Iteration: 94 Training Loss: 0.03791\n",
      "Epoch: 10/10 Iteration: 95 Validation accuracy: 0.85014\n",
      "Epoch: 10/10 Iteration: 95 Training Loss: 0.06205\n",
      "Epoch: 10/10 Iteration: 96 Training Loss: 0.04391\n",
      "Epoch: 10/10 Iteration: 97 Training Loss: 0.04289\n",
      "Epoch: 10/10 Iteration: 98 Training Loss: 0.03588\n",
      "Epoch: 10/10 Iteration: 99 Training Loss: 0.05311\n",
      "Epoch: 10/10 Iteration: 100 Validation accuracy: 0.84741\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "iteration = 0\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # TODO: Your training code here\n",
    "    for e in range(epochs):\n",
    "        for x, y in get_batches(train_x, train_y):\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict={inputs_: x, labels_: y})\n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Training Loss: {:.5f}\".format(loss))\n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 5 == 0:\n",
    "                valid_acc = sess.run(accuracy, feed_dict={inputs_: val_x, labels_: val_y})\n",
    "                print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Validation accuracy: {:.5f}\".format(valid_acc))\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/flowers.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Below you see the test accuracy. You can also see the predictions returned for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/flowers.ckpt\n",
      "Test accuracy: 0.8774\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    feed = {inputs_: test_x,\n",
    "            labels_: test_y}\n",
    "    test_acc = sess.run(accuracy, feed_dict=feed)\n",
    "    print(\"Test accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
