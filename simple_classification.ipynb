{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "  * [Simple Classification](https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/302_simple_classification.py)\n",
    "    1. 生成假資料\n",
    "      * 二群。群一以(2, 2)為中心，lable=0；群二以(-2, -2)為中心，lable=1\n",
    "      * 將群一與群二的資料合併, (x, y) = (坐標點, label)。\n",
    "    2. 畫出原始資料的分佈。\n",
    "    3. 建立神經網路\n",
    "      * L1 層的輸入為 x(節點數為 2 = 坐標), 輸出的節點數為 10, activity_func=tf.nn.relu\n",
    "      * output 層的輸入為 L1 的輸出(節點數為 10)，其輸出的節點數為 2，代表各為 lable=1或0 時的機率)\n",
    "      * 定義 loss\n",
    "      * 定義 accuracy\n",
    "      * 執行\n",
    "        * 注意 sess.run([train_op, accuracy, output], {tf_x: x, tf_y: y})，是指著並非所有定義的圖層都會被執行，而是只有這三個train_op, accuracy, output 圖層才會被執行而已\n",
    "          * 其 return 值分別為以上三個 op 的結果。\n",
    "          * 參考[session.run([fetch1, fetch2])](https://blog.csdn.net/u012436149/article/details/52908692)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# fake data\n",
    "n_data = np.ones((100, 2))\n",
    "x0 = np.random.normal(2*n_data, 1)      # class0 x shape=(100, 2)\n",
    "y0 = np.zeros(100)                      # class0 y shape=(100, )\n",
    "x1 = np.random.normal(-2*n_data, 1)     # class1 x shape=(100, 2)\n",
    "y1 = np.ones(100)                       # class1 y shape=(100, )\n",
    "x = np.vstack((x0, x1))  # shape (200, 2) + some noise\n",
    "y = np.hstack((y0, y1))  # shape (200, )\n",
    "\n",
    "# plot data\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, s=100, lw=0, cmap='RdYlGn')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "tf_x = tf.placeholder(tf.float32, x.shape)     # input x\n",
    "tf_y = tf.placeholder(tf.int32, y.shape)     # input y\n",
    "\n",
    "# neural network layers\n",
    "l1 = tf.layers.dense(tf_x, 10, tf.nn.relu)          # hidden layer\n",
    "output = tf.layers.dense(l1, 2)                     # output layer\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=tf_y, logits=output)           # compute cost\n",
    "accuracy = tf.metrics.accuracy(          # return (acc, update_op), and create 2 local variables\n",
    "    labels=tf.squeeze(tf_y), predictions=tf.argmax(output, axis=1),)[1]\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "sess = tf.Session()                                                                 # control training and others\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)     # initialize var in graph\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "for step in range(100):\n",
    "    # train and net output\n",
    "    _, acc, pred = sess.run([train_op, accuracy, output], {tf_x: x, tf_y: y})\n",
    "    if step % 20 == 0:\n",
    "        # plot and show learning process\n",
    "        plt.cla()\n",
    "        plt.scatter(x[:, 0], x[:, 1], c=pred.argmax(1), s=100, lw=0, cmap='RdYlGn')\n",
    "        plt.text(1.5, -4, 'Accuracy=%.2f' % acc, fontdict={'size': 20, 'color': 'red'})\n",
    "        plt.pause(0.1)\n",
    "\n",
    "# plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
